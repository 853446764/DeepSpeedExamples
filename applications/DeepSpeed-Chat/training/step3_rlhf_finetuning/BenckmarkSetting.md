# Benchmark setting used in [blog]() and [Landing Page]()
An apple-to-apple comparison is critical for machine learning community, particularly for benchmarking. As such, here we provide a very detailed instruction how we chose our setting, and encourge others to compare DeepSpeed-RLHF under the same setting or adjust DeepSpeed-RLHF based on their setting instead of directly grab the numbers and compare us under a different setting. 

We randomly select 40% training data from the six training datasets, i.e., ``Dahoas/rm-static Dahoas/full-hh-rlhf, Dahoas/synthetic-instruct-gptj-pairwise, yitingxie/rlhf-reward-datasets openai/webgpt_comparisons stanfordnlp/SHP.``. The total training samples we have here is 264292. We fix the query (prompt) sequence length as 256 and generate fixed-length answer with 256 tokens. As such, the total training tokens per epoch is 135,317,504. During benchmark testing, we set the training epoch number as 1.

As mentioned in instability of RLHF training ([Tutorial]()), we find outit is not stable to update the actor model for multiple times using the generated data. Therefore, we set ``per_device_train_batch_size=per_device_mini_batch_size`` and ``ppo_epochs=generation_batch_numbers=1`` for all our benchmark results. During testing, we also set a upper bound maximum global training tokens as 524,288 (batch size 1024 with sequence length 512). The reason is that this is the largest batch size during our exploration that can have stable RLHF training experience. Users and practitioners may find better training hyperparameters to further increase it. Also during testing, whenever the global training token batch size does not exceed our limit (523,288), we always use the largest training batch size with Out of Memory error to benchmark the time. 

We hope this clearly explained our benchmark setting and please do not hesistae to contact if you need more information.
